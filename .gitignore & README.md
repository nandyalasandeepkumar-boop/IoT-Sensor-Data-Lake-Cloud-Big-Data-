gitignore

# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.venv/
venv/
.env
*.ipynb_checkpoints

# Terraform
**/.terraform/*
*.tfstate
*.tfstate.*
crash.log
override.tf
override.tf.json
*.tfvars
*.tfvars.json

# Build artifacts
build/
dist/
*.zip


markdown

# IoT Sensor Data Lake (AWS)

End-to-end sample:
- **Ingest** simulated IoT data → AWS Lambda Function URL → S3 data lake (partitioned).
- **Storage**: S3 with lifecycle (hot → IA → Glacier → expire).
- **Query**: Athena external table (partition columns).
- **Analytics**: Python aggregation + simple anomaly detection.

## Architecture
Simulator (HTTP POST) → Lambda (Function URL) → S3 `raw/year=YYYY/month=MM/day=DD/hour=HH/device_id=.../event_*.json`
Athena reads `raw/` with partitions; analytics writes to `curated/`.

## Quick start

### 0) Prereqs
- AWS CLI configured (`aws configure`)
- Terraform ≥ 1.5
- Python ≥ 3.10

### 1) Deploy infra
```bash
cd infra/aws
terraform init
terraform apply -auto-approve -var="bucket_name=<YOUR_UNIQUE_BUCKET_NAME>" -var="aws_region=<YOUR_AWS_REGION>"
